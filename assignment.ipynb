{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd6a5138-f81a-47c9-852f-7673caf627c0",
   "metadata": {},
   "source": [
    "# Hands-On Assignment 0\n",
    "\n",
    "This will be a simple assignment to get you used to the environment and tools we will be using throughout this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba0f2f-7978-435a-862f-ca2f451960b7",
   "metadata": {},
   "source": [
    "## Part 0: Reading the Readme\n",
    "\n",
    "Before continuing with this notebook, first make sure to thoroughly read the `README.md` file for this assignment.\n",
    "The readme file contains useful information that you will use throughout all of your assignments.\n",
    "Refer back to this file whenever you have questions in future assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743757d5-df44-43ab-b174-55f284057c8e",
   "metadata": {},
   "source": [
    "## Part 1: iPython Notebooks\n",
    "\n",
    "This file is an iPython notebook, which allows a mix of textual content (with $\\LaTeX$, images, and HTML), editable code, and code output in the browser.\n",
    "\n",
    "For this assignment, our goal is to make sure you can run notebooks, edit them, and submit them as assignments for CSE 40.\n",
    "In each hands-on assignment, you will be asked to complete tasks in a notebook that complement the lectures and teach you about the basics of machine learning and data science.\n",
    "After you complete each assignment, you must submit your notebook by running `python3 -m autograder.run.submit assignment.ipynb` from your local repository.\n",
    "\n",
    "### Why Notebooks?\n",
    "\n",
    "Notebooks such as this one have found widespread use for sharing code examples and instruction, especially in machine learning.\n",
    "They are not suited to larger software projects, but for short assignments such as the ones in this class, they can work well.\n",
    "\n",
    "### About Notebook for Assignments\n",
    "\n",
    "Generally, notebooks are organized into \"cells\" that either contain HTML\n",
    "(usually rendered from [markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) with optional [math](https://www.mathjax.org/#demo)),\n",
    "source code (e.g., Python snippets), or code outputs. \n",
    "\n",
    "For hands-on assignments, you will be asked to complete \"Tasks\", denoted with an <span style=\"color: darkorange; font-size: x-large;\">orange star (★)</span>,\n",
    "usually involving the editing of Python source code cells, which can then be run interactively to generate output in the same notebook.\n",
    "\n",
    "Be sure to refer to [official resources](https://jupyterlab.readthedocs.io/en/stable/) on [iPython notebooks](https://jupyterlab.readthedocs.io/en/stable/user/notebook.html)\n",
    "or the [Jupyter Lab interface](https://jupyterlab.readthedocs.io/en/stable/user/interface.html) for specific questions that you may have.\n",
    "\n",
    "There will be some quirks and patterns that come with using notebooks for assignments that you will have to get used to.\n",
    "For example, all imports should always go in the first part of the first code cell in a notebook\n",
    "(as you can see in the next cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3382c1a7-3ea3-4401-9e43-7cc8a38bac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the first code cell of the notebook,\n",
    "# so all imports should go here.\n",
    "# These imports will be used later in this assignment,\n",
    "# so make sure to run this cell.\n",
    "\n",
    "import types\n",
    "\n",
    "import autograder.question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7481677-d99b-484b-9429-112d10c87453",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkorange; font-size: x-large\";>★ Task 1.A</h3>\n",
    "\n",
    "Edit the following function to return `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e543a4b-2655-4da9-a4f8-b57269d8d2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function():\n",
    "    \"\"\"\n",
    "    The output of this function will be tested (it must return True).\n",
    "    \"\"\"\n",
    "\n",
    "    return NotImplemented\n",
    "\n",
    "my_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b4605d-578b-4ea6-966b-edb3e7022e6d",
   "metadata": {},
   "source": [
    "After editing `my_function()`, run the above code cell (CTRL+Enter).\n",
    "Running the cell both defines the function (the `def` part) and runs it (the last line in the cell).\n",
    "Note that by default the last value declared in a cell is printed as output.\n",
    "\n",
    "The above cell should now return `True` instead of `NotImplemented` or raising an exception.\n",
    "Most (but not all) functions you will be asked to implement in the future will provide an implementation that runs,\n",
    "but does not produce the correct output.\n",
    "You will always be expected to edit the functions to return the correct results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393f8f2e-f074-4709-99fb-755cbd966fc2",
   "metadata": {},
   "source": [
    "## Part 2: Testing\n",
    "\n",
    "As you have probably already experienced, testing the code we write is critical to the software development process.\n",
    "We can always test our code by just running it and looking at the outputs it produces.\n",
    "However, more disciplined [software testing](https://en.wikipedia.org/wiki/Software_testing) becomes more and more import as our code gets larger and more complex.\n",
    "Specifically, we want to write code that can be used to test our other code.\n",
    "We call code that tests specific pieces/units of our code (e.g. functions, classes, methods, etc) [unit tests](https://en.wikipedia.org/wiki/Unit_testing).\n",
    "\n",
    "For example, we may want to create a super simple function that rounds a number to its closest integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e535ac9-fe21-447d-bf47-01e49ab90e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_rounding_function(x):\n",
    "    return int(x + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59ecac9-ea23-4e90-b566-505ed9602123",
   "metadata": {},
   "source": [
    "You may have already spotted that our function has a bug in it,\n",
    "but how can we write code to check if our function is correct?\n",
    "In the rest of this part, we will talk about two of the many ways we can write unit tests.\n",
    "We will use these methods to write tests for our super simple function,\n",
    "while you will use these methods to write tests for Task 1.A.\n",
    "You should apply these testing concepts to all future assignments in this course, at UCSC, and in your career as someone who writes code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f258c038-a855-4e0e-848c-b344fb57f174",
   "metadata": {},
   "source": [
    "### Notebook Tests\n",
    "\n",
    "One way to write your own tests is to write them directly inside this notebook.\n",
    "You can make a new cell that has code to test your other code.\n",
    "When writing tests (especially unit tests), you will usually use some testing framework\n",
    "(like [PyTest](https://docs.pytest.org) or Python's [built-in testing library](https://docs.python.org/3/library/unittest.html)).\n",
    "But for writing quick and informal tests in the notebook,\n",
    "you can also just make use of prints or Python's built-in [assert statement](https://docs.python.org/3/reference/simple_stmts.html#assert).\n",
    "The assert statement is used before an expression.\n",
    "If the expression is `True`, then nothing happens.\n",
    "If the expression is `False`, then an [AssertionError](https://docs.python.org/3/library/exceptions.html#AssertionError) is raised.\n",
    "\n",
    "For example, in the following cell we have a very simple example where we want to compare two numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1233707f-5793-4fc4-a18b-cf2087fbba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = 1\n",
    "\n",
    "assert a == b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b026740-5351-4283-8469-aeb1a3ada4a5",
   "metadata": {},
   "source": [
    "Since the variables contain the same value and our expression (`a == b`) is true, the assert statement does not do anything.\n",
    "\n",
    "But if we make our expression (`a == b`) false, then the assert statement will raise an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc656d24-3bb9-4859-b528-dc75643afcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = 2\n",
    "\n",
    "assert a == b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3073617f-f376-4ca5-b96d-71329129851a",
   "metadata": {},
   "source": [
    "You can also include an optional string after the expression to help describe your error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf031537-2025-4ec2-b730-9977fd0f2e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = 2\n",
    "\n",
    "assert a == b, 'One does not equal two!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f0835c-5cdb-4520-b78c-13dd5a4f5f63",
   "metadata": {},
   "source": [
    "Now we can now use assertions to help us test `my_rounding_function()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c4dbf-a9fa-4d97-b339-faf1d519d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_my_rounding_function():\n",
    "    # Test cases that we manually came up with organized in (input, expected output) pairs.\n",
    "    # Coming up with test cases is a great time to use pen and paper.\n",
    "    test_cases = [\n",
    "        (0.75, 1),\n",
    "        (0.50, 1),\n",
    "        (0.49, 0),\n",
    "        (1.00, 1),\n",
    "        (0.00, 0),\n",
    "        (-0.75, -1),\n",
    "    ]\n",
    "    \n",
    "    for (input_value, expected_output) in test_cases:\n",
    "        actual_output = my_rounding_function(input_value)\n",
    "        \n",
    "        message = \"Input: %f, Expected Output: %d, Actual Output: %d\" % (\n",
    "            input_value, expected_output, actual_output)\n",
    "        assert actual_output == expected_output, message\n",
    "\n",
    "# Run our new testing function.\n",
    "test_my_rounding_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e9ba0-63f5-4a01-962b-a43411fd3035",
   "metadata": {},
   "source": [
    "Looks like we found our bug!\n",
    "We can examine the output and see exactly which test case caused our function to output a bad answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ea0f8c-7a5a-44e3-859f-97a302b889c2",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkorange; font-size: x-large\";>★ Task 2.A</h3>\n",
    "\n",
    "Complete the following function that raises an exception when the passed in value is not a correct return value from `my_function()`.\n",
    "(Refer back to Task 1.A for information on what `my_function()` should return.)\n",
    "You can use an `assert` statement to raise an exception\n",
    "(it will raise an [AssertionError](https://docs.python.org/3/library/exceptions.html#AssertionError))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05b701-8dc2-4d63-9cb8-1b7ebf85f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_my_function_value(value):\n",
    "    # The pass statement in Python just says that there is nothing here (it is a no-op).\n",
    "    # Replace it with your actual code.\n",
    "    pass\n",
    "\n",
    "# Run your new testing function with the result from my_function().\n",
    "test_my_function_value(my_function())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00252a18-22d6-40b1-a6f1-86dbde86f2c8",
   "metadata": {},
   "source": [
    "What about when your code is run with a known bad value?\n",
    "Does it raise an exception like it is supposed to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5884bba7-35a2-4dea-8449-1057f58605e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_my_function_value(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bab6b1a-1582-41e9-b6ae-55b2c51aa41e",
   "metadata": {},
   "source": [
    "### The Local Grader\n",
    "\n",
    "With each assignment, we will provide you with a script that we call the \"local grader\".\n",
    "The local grader will always be located in a file called `local_grader.py`,\n",
    "and it runs tests in the exact same way as the autograder\n",
    "(which assigns your actual grade for assignments and will be discussed in the next part of this assignment).\n",
    "The only difference is that the autograder is on a secure server and keeps its test cases secret.\n",
    "Therefore, running the local grader is a great way to get a feel for how the autograder behaves.\n",
    "\n",
    "The infrastructure that the graders (both local and auto) use is publicly available in the [ucsc-cse40 package](https://github.com/ucsc-cse-40/ucsc-cse40).\n",
    "So if you are ever interested in how we are doing something, you can see for yourself.\n",
    "The local grader we provide you with only starts with **very** basic tests,\n",
    "but we encourage you to expand it with your own tests.\n",
    "\n",
    "Although designed for more robust testing,\n",
    "we can use the same tools that the local grader uses to test our simple `my_rounding_function()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d33c1-064f-48ce-a45e-22dd68c3e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMyRoundingFunction(autograder.question.Question):\n",
    "    def score_question(self, submission):\n",
    "        # The submission object contains all the code we are testing.\n",
    "        # For the graders, this usually means all the code cells in this notebook.\n",
    "        # In this case, it will just have our my_rounding_function() function.\n",
    "        \n",
    "        # We can use a very similar structure to the previous test we wrote\n",
    "        # (test_my_rounding_function()).\n",
    "        test_cases = [\n",
    "            (0.75, 1),\n",
    "            (0.50, 1),\n",
    "            (0.49, 0),\n",
    "            (1.00, 1),\n",
    "            (0.00, 0),\n",
    "            (-0.75, -1),\n",
    "        ]\n",
    "\n",
    "        missed_test_cases = 0\n",
    "        for (input_value, expected_output) in test_cases:\n",
    "            actual_output = submission.my_rounding_function(input_value)\n",
    "            if (actual_output != expected_output):\n",
    "                # Instead of raising an error as soon as a bad test case is encountered,\n",
    "                # we can just keep track of the test cases we missed.\n",
    "                missed_test_cases += 1\n",
    "                \n",
    "                message = \"Error -- Input: %f, Expected Output: %d, Actual Output: %d\" % (\n",
    "                    input_value, expected_output, actual_output)\n",
    "                self.add_message(message)\n",
    "        \n",
    "        # Subtract one point from our total score for each test case missed.\n",
    "        self.set_score(self.max_points - missed_test_cases)\n",
    "\n",
    "# Construct the question with a number of max points and name.\n",
    "question = TestMyRoundingFunction(6, \"Test my_rounding_function()\")\n",
    "\n",
    "# Prepare the submission object.\n",
    "# Normally this is more complex,\n",
    "# but when testing inside the notebook we can just point directly to the function we are testing.\n",
    "submission = types.SimpleNamespace(my_rounding_function = my_rounding_function)\n",
    "\n",
    "# Run the testing/scoring code.\n",
    "result = question.grade(submission)\n",
    "\n",
    "# Output the results.\n",
    "print(result.scoring_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d862fc-74ac-4ffb-8b22-d81248bec027",
   "metadata": {},
   "source": [
    "Here we can see that our test reports that only 5 of the 6 test cases ran successfully,\n",
    "and we can also see a nice error message giving us the details of the failing test case.\n",
    "\n",
    "Writing unit tests using tools like this takes longer and is typically more verbose than our simple testing function (`test_my_function_value()`),\n",
    "but it also gives us much more flexibility and functionality that will come in handy as we write more tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8950ab-4b55-4b38-8f27-210b42f7cce3",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkorange; font-size: x-large\";>★ Task 2.B</h3>\n",
    "\n",
    "Complete the following testing class.\n",
    "This class is copied over (except for the name) from the local grader for this assignment.\n",
    "Currently, it only checks that the result returned from `my_function()` is not `NotImplemented` and is a boolean.\n",
    "Add to this test class so that is will only get full credit (`self.full_credit()`) when `my_function()` is correct.\n",
    "Otherwise, the test case should fail (`self.fail()`).\n",
    "\n",
    "Note that normally the `submission` object is more complicated\n",
    "(since it will be an object that represents all the code you submit for an assignment),\n",
    "but since we are testing inside our notebook (instead of in a different script/file)\n",
    "we can just point to the function we are testing.\n",
    "\n",
    "*Hint: If you are getting a `NameError` (like `NameError: name 'cse40' is not defined`),\n",
    "then you may not have run the import statements in the first code cell of this assignment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2021314f-468d-49f1-8f97-6b91a7a508a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMyFunction(autograder.question.Question):\n",
    "    def score_question(self, submission):\n",
    "        # We cal call your code using the submission object.\n",
    "        result = submission.my_function()\n",
    "        \n",
    "        # Does the function return NotImplemented?\n",
    "        if (self.check_not_implemented(result)):\n",
    "            return\n",
    "\n",
    "        # Does the function return a boolean?\n",
    "        if (not isinstance(result, bool)):\n",
    "            self.fail(\"Function must return a boolean value.\")\n",
    "            return\n",
    "\n",
    "        # Add your code here.\n",
    "        \n",
    "        self.full_credit()\n",
    "\n",
    "# Construct the question with a number of max points and a name (neither matter in this case).\n",
    "question = TestMyFunction(100, \"Test my_function()\")\n",
    "\n",
    "# Prepare the submission object.\n",
    "# Normally this is more complex,\n",
    "# but when testing inside the notebook we can just point directly to the function we are testing.\n",
    "submission = types.SimpleNamespace(my_function = my_function)\n",
    "\n",
    "# Run the testing/scoring code.\n",
    "result = question.grade(submission)\n",
    "\n",
    "# Output the results.\n",
    "print(result.scoring_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07133bb-6a6f-4404-9cfe-de3dd278ccc0",
   "metadata": {},
   "source": [
    "## Part 3: The Autograder\n",
    "\n",
    "All your hands-on assignments will be graded by a system we call \"the autograder\".\n",
    "As mentioned in Part 2, the autograder uses the same testing infrastructure that the local grader uses.\n",
    "The only difference is that the autograder is run on a secure server (instead of your local machine)\n",
    "and the autograder has hidden tests that you cannot see (instead of just the tests located in `local_grader.py`).\n",
    "\n",
    "Once you submit an assignment to the autograder,\n",
    "it will test your code against secret test cases written by the TAs and assign your submission a score which it will send back to you.\n",
    "You can make as many attempts/submissions as you want.\n",
    "However, you should make sure to not abuse the autograder by using is as a complier or style checker.\n",
    "You should always run your local tests and the local grader before submitting to the autograder.\n",
    "When the autograder does not give you full credit for a task,\n",
    "try making a test case that is testing the same thing the autograder is and run your new test locally.\n",
    "Remember, the autograder is a shared resource for all students and you should be respectful by testing locally first.\n",
    "\n",
    "The score for your most recent submission is considered your current score on the assignment\n",
    "(ignoring late days and any manually graded components).\n",
    "Be careful when submitting past the assignment's deadline, because you will start using your late days.\n",
    "The current grade the autograder assigns you is considered official and will eventually be reflected on Canvas\n",
    "(which will be regularly updated with your most recent submission).\n",
    "\n",
    "### Interacting with the Autograder\n",
    "\n",
    "All interaction with the autograder is done using the `autograder-py` package included in the `ucsc-cse40` package.\n",
    "Once the `ucsc-cse40` package is installed on your machine, you can invoke the tools using `python3 -m`.\n",
    "See [the documentation](https://github.com/eriq-augustine/autograder-py) for a full list of all the autograder tools.\n",
    "\n",
    "You can list out all the available tools using:\n",
    "```\n",
    "python3 -m autograder.run\n",
    "```\n",
    "\n",
    "You can also get specific help for each tool using `--help`:\n",
    "```\n",
    "python3 -m autograder.run.submit --help\n",
    "```\n",
    "\n",
    "The tool has options to configure how it runs (as seen in the help prompt),\n",
    "but as long as you run the tool in your assignment directory (the one that this file lives in)\n",
    "then all the default options should work just fine.\n",
    "\n",
    "For this course, you will only need to use commands in the `autograder.run` package of shortcuts.\n",
    "But, if you want to see ALL the commands available to you,\n",
    "you can look into the `autograder.cli` package:\n",
    "```sh\n",
    "python3 -m autograder.cli -r\n",
    "```\n",
    "\n",
    "#### Submitting\n",
    "\n",
    "To submit your code, you can use the `submit` command:\n",
    "```\n",
    "python3 -m autograder.run.submit assignment.ipynb\n",
    "```\n",
    "\n",
    "If the grading was successful, then you will see output that is very similar to the local grader.\n",
    "For example, you may see output like:\n",
    "```\n",
    "The autograder successfully graded your assignment.\n",
    "Autograder transcript for project: Hands-On 0: Getting Started.\n",
    "Grading started at 2023-03-30 17:57 and ended at 2023-03-30 17:57.\n",
    "Q1: 0 / 100\n",
    "   NotImplemented returned.\n",
    "Style: 0 / 0\n",
    "   Style is clean!\n",
    "\n",
    "Total: 0 / 100\n",
    "```\n",
    "\n",
    "Your score on the assignment is determined by your most recent submission to the autograder.\n",
    "You can see the exact time the autograder accepted your submission by looking at the grading report it outputs.\n",
    "The time after \"Grading started at\" is your official submission time and will be used to compute any late days.\n",
    "\n",
    "Note that this submission step is **required** for getting a grade for hands-on assignments.\n",
    "Just pushing your code in git (as some classes do) is not sufficient for receiving a grade.\n",
    "Since pushing code and getting graded are not linked in this class,\n",
    "we encourage you to commit and push your code in git often.\n",
    "\n",
    "For HO0, no points are awarded for having correct coding style,\n",
    "but that will change starting with HO1.\n",
    "You may have already noted that the local grader also runs style checks.\n",
    "You should make sure your style passes the local grader before submitting to the autograder\n",
    "(they check your code's style using the same infrastructure from the `ucsc-cse40` package).\n",
    "\n",
    "#### Checking Your Last Score\n",
    "\n",
    "You can ask the autograder to show you your last submission using the `peek` command:\n",
    "```\n",
    "python3 -m autograder.run.peek\n",
    "```\n",
    "\n",
    "If the lookup was successful, then you will see output that is very similar to when you submitted your code originally.\n",
    "For example, you may see output like:\n",
    "```\n",
    "The autograder successfully found your last attempt for this assignment.\n",
    "Autograder transcript for project: Hands-On 0: Getting Started.\n",
    "Grading started at 2023-03-30 17:57 and ended at 2023-03-30 17:57.\n",
    "Q1: 0 / 100\n",
    "   NotImplemented returned.\n",
    "Style: 0 / 0\n",
    "   Style is clean!\n",
    "\n",
    "Total: 0 / 100\n",
    "```\n",
    "\n",
    "#### Checking Your Score History\n",
    "\n",
    "To check all your previous scores for this assignment, you can use the `history` command:\n",
    "```\n",
    "python3 -m autograder.run.history\n",
    "```\n",
    "\n",
    "This command will return a summary of your past submissions, like:\n",
    "```\n",
    "Found 2 submissions.\n",
    "    Submission ID: 1695682455, Score: 24 / 100, Time: 2023-09-25 17:54.\n",
    "    Submission ID: 1695735313, Score: 100 / 100, Time: 2023-09-26 08:35, Message: 'I did it!'\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
